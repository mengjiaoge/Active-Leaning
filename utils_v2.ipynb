{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-4-e6e9df17af8a>, line 8)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-e6e9df17af8a>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    nltk.download(['stopwords', 'punkt', 'brown', 'averaged_perceptron_tagger])\u001b[0m\n\u001b[1;37m                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# importing required packages\n",
    "\n",
    "import nltk\n",
    "nltk.download(['stopwords', 'punkt', 'brown', 'averaged_perceptron_tagger', 'wordnet'])\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re, collections\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence into words\n",
    "\n",
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing an essay into a list of word lists\n",
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating average word length in an essay\n",
    "\n",
    "def avg_word_len(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of words in an essay\n",
    "\n",
    "def word_count(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of characters in an essay\n",
    "\n",
    "def char_count(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\s', '', str(essay).lower())\n",
    "    \n",
    "    return len(clean_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of sentences in an essay\n",
    "\n",
    "def sent_count(essay):\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of lemmas per essay\n",
    "\n",
    "def count_lemmas(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)      \n",
    "    \n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence) \n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "        \n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking number of misspelled words\n",
    "\n",
    "def count_spell_error(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', str(essay).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "    \n",
    "    #word.txt: It is a concatenation of public domain book excerpts from Project Gutenberg \n",
    "    #         and lists of most frequent words from Wiktionary and the British National Corpus.\n",
    "    #         It contains about a million words.\n",
    "    data = open('word.txt').read()\n",
    "    \n",
    "    words_ = re.findall('[a-z]+', data.lower())\n",
    "    \n",
    "    word_dict = collections.defaultdict(lambda: 0)\n",
    "                       \n",
    "    for word in words_:\n",
    "        word_dict[word] += 1\n",
    "                       \n",
    "    clean_essay = re.sub(r'\\W', ' ', str(essay).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "                        \n",
    "    mispell_count = 0\n",
    "    \n",
    "    words = clean_essay.split()\n",
    "                        \n",
    "    for word in words:\n",
    "        if not word in word_dict:\n",
    "            mispell_count += 1\n",
    "    \n",
    "    return mispell_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of nouns, adjectives, verbs and adverbs in an essay\n",
    "\n",
    "def count_pos(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "            \n",
    "    return noun_count, adj_count, verb_count, adv_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting essay features\n",
    "\n",
    "def extract_features(data):\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    features['char_count'] = features['EssayText'].apply(char_count)\n",
    "    \n",
    "    features['word_count'] = features['EssayText'].apply(word_count)\n",
    "    \n",
    "    features['sent_count'] = features['EssayText'].apply(sent_count)\n",
    "    \n",
    "    features['avg_word_len'] = features['EssayText'].apply(avg_word_len)\n",
    "    \n",
    "    features['lemma_count'] = features['EssayText'].apply(count_lemmas)\n",
    "    \n",
    "    features['spell_err_count'] = features['EssayText'].apply(count_spell_error)\n",
    "    \n",
    "    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['EssayText'].map(count_pos))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getiing tfidf\n",
    "\n",
    "def get_tfidf_vectors(essays):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9, min_df=5,\\\n",
    "                                 max_features=400, stop_words=\"english\", binary=True)\n",
    "    \n",
    "    tfidf_vectors = vectorizer.fit_transform(essays)\n",
    "    \n",
    "    return pd.DataFrame(tfidf_vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "#     return pd.concat([df, new_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getiing Bag of Words (BOW) counts\n",
    "\n",
    "def get_count_vectors(essays):\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features = 10000, ngram_range=(1, 3), stop_words='english')\n",
    "    \n",
    "    count_vectors = vectorizer.fit_transform(essays)\n",
    "    \n",
    "    return pd.DataFrame(count_vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "#     return feature_names, count_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
