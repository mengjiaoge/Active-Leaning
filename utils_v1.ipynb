{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "appropriate-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "from string import printable\n",
    "# from string import lower\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cleared-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"data/train.tsv\"\n",
    "TEST_PATH = \"data/public_leaderboard.tsv\"\n",
    "\n",
    "NA_STRING = \"NA\"\n",
    "ESSAY_SETS = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "shaped-finding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\menge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\menge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\menge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(['stopwords', 'punkt', 'brown', 'averaged_perceptron_tagger])\n",
    "STOPWORDS = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords_set = set(STOPWORDS)\n",
    "wn = nltk.corpus.wordnet\n",
    "brown = nltk.corpus.brown\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "printable_characters = set([k for k in printable if k not in (\"\\n\",\"\\t\",\"\\r\",\";\",'\"',\"'\")])\n",
    "word_stat = nltk.FreqDist([w.lower() for w in brown.words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "reported-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAY_TRAIN = 1\n",
    "ESSAY_PUBLIC = 2\n",
    "\n",
    "\n",
    "class Essays():\n",
    "    def __init__(self, train_path = TRAIN_PATH, test_path = TEST_PATH):\n",
    "        input = csv.reader(open(train_path),delimiter=\"\\t\")\n",
    "        essays = [line for line in input][1:]\n",
    "        self.essays = [Essay(int(line[0]), int(line[1]), int(line[2]), int(line[3]), line[4], ESSAY_TRAIN) for line in essays]\n",
    "\n",
    "        if test_path:\n",
    "            input = csv.reader(open(test_path),delimiter=\"\\t\")\n",
    "            essays = [line for line in input][1:]\n",
    "            self.essays.extend([Essay(int(line[0]), int(line[1]), None, None, line[2], ESSAY_PUBLIC) for line in essays])\n",
    "\n",
    "\n",
    "    def apply_text_function(self, source, destination, f, essay_set = None):\n",
    "        #print time.asctime(time.localtime())\n",
    "        for n in range(len(self.essays)):\n",
    "            if essay_set == None or self.essays[n].essay_set == essay_set:\n",
    "                processed_version = f(self.essays[n].get_text(source))\n",
    "                self.essays[n].set_text(destination, processed_version)\n",
    "\n",
    "    def add_feature(self, feature_name, feature_extract_f, filter_function = None):\n",
    "        for n in range(len(self.essays)):\n",
    "            if filter_function == None or filter_function(self.essays[n]):\n",
    "                self.essays[n].features[feature_name] = feature_extract_f(self.essays[n])\n",
    "\n",
    "    def get_essay_by_id(self,essay_id):\n",
    "        return [essay for essay in self.essays if essay.id == essay_id][0]\n",
    "\n",
    "    def get_essay_set(self,essay_set):\n",
    "        return [essay for essay in self.essays if essay.essay_set == essay_set]\n",
    "\n",
    "    def get_essays_by_type(essay_type):\n",
    "        return [essay for essay in self.essays if essay.essay_type == essay_type]\n",
    "\n",
    "    def get_essay_set_possible_scores(self,essay_set):\n",
    "        return list(set([essay for essay in self.essays if essay.essay_set == essay_set]))\n",
    "\n",
    "    def get_corpora_by_essay_set(self,essay_set,version):\n",
    "        corpora = {}\n",
    "        for essay in self.get_essay_set(essay_set):\n",
    "            if essay.essay_type == ESSAY_TRAIN:\n",
    "                for word in essay.get_text(version).split():\n",
    "                    if word in corpora:\n",
    "                        corpora[word] += 1\n",
    "                    else:\n",
    "                        corpora[word] = 1\n",
    "        return corpora\n",
    "\n",
    "    def get_data(self,essay_set = None):\n",
    "        features_names = []\n",
    "        for essay in self.essays:\n",
    "            if essay_set == None or essay.essay_set == essay_set:\n",
    "                features_names += essay.get_features_names()\n",
    "        features_names = sorted(list(set(features_names)))\n",
    "\n",
    "        data = []\n",
    "        data.append([\"id\",\"essay_set\",\"score_1\",\"score_2\"] + features_names)\n",
    "        \n",
    "        for essay in self.essays:\n",
    "            if essay_set == None or essay.essay_set == essay_set:\n",
    "                row = []\n",
    "                for feature in features_names:\n",
    "                    row.append(essay.get_feature(feature))\n",
    "                data.append([essay.id, \"Essay\" + str(essay.essay_set), essay.score_1, essay.score_2] + row)\n",
    "        return data\n",
    "\n",
    "    def load_text_from_file(self, path, text_version):\n",
    "        inp = open(path)\n",
    "        for essay_ind, line in enumerate(inp.readlines()):\n",
    "            line_split = line.split(\"\\t\")\n",
    "            text = line_split[1].replace(\"\\n\",\"\")\n",
    "            self.essays[essay_ind].set_text(text_version, text)\n",
    "        inp.close()\n",
    "                        \n",
    "class Essay():\n",
    "    def __init__(self, id, essay_set, score_1, score_2, raw_text, essay_type):\n",
    "        self.id = id\n",
    "        self.essay_set = essay_set\n",
    "        self.score_1 = score_1\n",
    "        self.score_2 = score_2\n",
    "        self.text_versions = {}\n",
    "        self.set_text(\"raw\",raw_text)\n",
    "        self.essay_type = essay_type\n",
    "        self.features = {}\n",
    "\n",
    "    def get_tokens(self, version):\n",
    "        return self.get_text(version).split()\n",
    "\n",
    "    def set_text(self, version, text):\n",
    "        self.text_versions[version] = text\n",
    "\n",
    "    def get_text(self, version):\n",
    "        return self.text_versions.get(version,\"\")\n",
    "\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "\n",
    "    def get_feature(self,name):\n",
    "        return self.features.get(name, NA_STRING)\n",
    "\n",
    "    def get_features_names(self):\n",
    "        return self.features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-universal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-guard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-colleague",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-knight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-story",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Essay():\n",
    "    def __init__(self, id, essay_set, score_1, score_2, raw_text, essay_type):\n",
    "        self.id = id\n",
    "        self.essay_set = essay_set\n",
    "        self.score_1 = score_1\n",
    "        self.score_2 = score_2\n",
    "        self.text_versions = {}\n",
    "        self.set_text(\"raw\",raw_text)\n",
    "        self.essay_type = essay_type\n",
    "        self.features = {}\n",
    "\n",
    "    def get_tokens(self, version):\n",
    "        return self.get_text(version).split()\n",
    "\n",
    "    def set_text(self, version, text):\n",
    "        self.text_versions[version] = text\n",
    "\n",
    "    def get_text(self, version):\n",
    "        return self.text_versions.get(version,\"\")\n",
    "\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "\n",
    "    def get_feature(self,name):\n",
    "        return self.features.get(name, NA_STRING)\n",
    "\n",
    "    def get_features_names(self):\n",
    "        return self.features.keys()\n",
    "\n",
    "# gets the text off all essays\n",
    "def get_combined_text(essays,version,filter_function = None):\n",
    "    return [essay.get_text(version) for essay in essays if filter_function == None or filter_function(essay)]\n",
    "        \n",
    "# text functions\n",
    "def remove_all_non_printable(text):\n",
    "    return \"\".join([k for k in text if k in printable_characters])\n",
    "\n",
    "def remove_all_non_characters(text):\n",
    "    return re.sub(\"[^a-zA-Z\\s]\",\" \",text)\n",
    "\n",
    "def remove_non_number(text):\n",
    "    return re.sub(\"[^0-9\\s]\",\" \",text).strip()\n",
    "\n",
    "def remove_multispaces(text):\n",
    "    return re.sub(\"[\\s]+\",\" \",text)\n",
    "\n",
    "def remove_spaces(text):\n",
    "    return re.sub(\"[\\s]\",\"\",text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([w for w in text.split() if w not in STOPWORDS])\n",
    "\n",
    "def replace_consecutive_letters(text):\n",
    "    letters = [k for k in text if k not in [\" \",]]\n",
    "    for letter in letters:\n",
    "        text = re.sub(\"[%s]+\" % (letter,), letter, text)\n",
    "    return text\n",
    "\n",
    "def count_words_with_len(words,lengths):\n",
    "    return len([word for word in words if len(word) in lengths])\n",
    "\n",
    "def count_words_with_geq_len(words,length):\n",
    "    return len([word for word in words if len(word) >= length])\n",
    "\n",
    "def porter_stemmer(text):\n",
    "    return \" \".join([nltk.stem.porter.PorterStemmer().stem(word) for word in text.split()])\n",
    "\n",
    "def pos_tagger(text):\n",
    "    text = text.split()\n",
    "    return [word[1] for word in nltk.pos_tag(text)]\n",
    "\n",
    "def mistaken_words(text):\n",
    "    mistakes = []\n",
    "    for word in text.split():\n",
    "        if not spellchecker.check(word):\n",
    "            mistakes.append(word)\n",
    "    return mistakes\n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([k for k in text.split() if k not in STOPWORDS])\n",
    "\n",
    "def spellcheck(text):\n",
    "    correct_text = text\n",
    "    for word in text.split():\n",
    "        if not spellchecker.check(word):\n",
    "            suggestion = spellchecker.suggest(word)\n",
    "            if suggestion:\n",
    "                correct_text = correct_text.replace(word,suggestion[0])\n",
    "    return correct_text\n",
    "\n",
    "def remove_rare_tokens(corpora,sparsity = 1):\n",
    "    def process(text):\n",
    "        tokens = text.split()\n",
    "        for word in tokens:\n",
    "            if corpora.get(word,0) <= sparsity:\n",
    "                tokens.remove(word)\n",
    "        return \" \".join(tokens)\n",
    "    return process\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return sentence_tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "def add_definitions(text,used_dictionary=None):\n",
    "    rawtext_adj = \"\"\n",
    " \n",
    "    # Print the information\n",
    "    for word in text.split():\n",
    "        rawtext_adj += \" \" + word\n",
    "        if len(word) >= 4:\n",
    "            synsets = wn.synsets(word)\n",
    "            for synset in synsets:\n",
    "                #print \"-\" * 10\n",
    "                #print \"Name:\", synset.name\n",
    "                #print \"Lexical Type:\", synset.lexname\n",
    "                #print \"Lemmas:\", synset.lemma_names\n",
    "                #print \"Definition:\", synset.definition\n",
    "                #for example in synset.examples:\n",
    "                #    print \"Example:\", example\n",
    "                rawtext_adj += \" \" + \" \".join(synset.lemma_names)\n",
    "                rawtext_adj += \" \" + synset.definition\n",
    "    return rawtext_adj\n",
    "\n",
    "\n",
    "@memoized\n",
    "def wup_similarity(a,b):\n",
    "    return wn.wup_similarity(a,b)\n",
    "\n",
    "# utility functions\n",
    "def export_data_sep_files(essays):\n",
    "    for essay_set in ESSAY_SETS:\n",
    "        data = essays.get_data(essay_set)\n",
    "        write_data(data,\"data_%d.csv\" % (essay_set,))\n",
    "\n",
    "def export_data_one_file(essays):\n",
    "    data = essays.get_data()\n",
    "    write_data(data,\"data.csv\")\n",
    "    \n",
    "def write_data(data, path):\n",
    "    output = open(path, \"w\")\n",
    "    for row in data:\n",
    "        output.write(\";\".join([str(k) for k in row]) + \"\\n\")\n",
    "    output.close()\n",
    "\n",
    "def export_text(essays, version, path, save_id = True):\n",
    "    output = open(path, \"w\")\n",
    "    for essay in essays:\n",
    "        if save_id:\n",
    "            output.write(\"%s\\t%s\\n\" % (essay.id, essay.get_text(version)))\n",
    "        else:\n",
    "            output.write(\"%s\\n\" % (essay.get_text(version)))\n",
    "    output.close()\n",
    "\n",
    "def export_feature(feature, path):\n",
    "    output = open(path, \"w\")\n",
    "    for elem in feature:\n",
    "        output.write(\"%s\\n\" % (elem,))\n",
    "    output.close()\n",
    "\n",
    "# cuts the text into list of 10 tokens each\n",
    "def cut_text(text,n=10):\n",
    "    splited = text.split()\n",
    "    return [' '.join(splited[x:x+n]) for x in xrange(0, len(splited), n)]\n",
    "    \n",
    "# calculates cosinus distance\n",
    "def cosine_distance(v1, v2):\n",
    "    return float(np.dot(v1,v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "# returns word synonyms\n",
    "def synonyms(word):\n",
    "    return [l for s in wn.synsets(word) for l in s.lemmas]\n",
    "\n",
    "# splits the text into ngrams\n",
    "def get_ngrams(text,n):\n",
    "    text_split = text.split()\n",
    "    ngrams = []\n",
    "    for ind in range(0,len(text_split)-n+1):\n",
    "        ngrams.append(text_split[ind:ind+n])\n",
    "    return ngrams\n",
    "\n",
    "def sign(n):\n",
    "    if n==0:\n",
    "        return 0\n",
    "    elif n < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def hashingtrick(document,M):\n",
    "    bow=[0]*M\n",
    "    for word in document.split():\n",
    "        h=word.__hash__()\n",
    "        bow[abs(h) % M]+=sign(h)\n",
    "    return bow\n",
    "\n",
    "class NgramFreq():\n",
    "    def __init__(self, essays, n):\n",
    "        self.essays = essays\n",
    "        self.n = n\n",
    "        self.freq = self.get_ngrams_dict()\n",
    "\n",
    "    def get_ngrams_dict(self):\n",
    "        self.ngrams_dict = {}\n",
    "        for essay in self.essays:\n",
    "            ngrams = get_ngrams(essay,self.n)\n",
    "            for ngram in ngrams:\n",
    "                ngram_key = \"###\".join(ngram)\n",
    "                if ngram_key in self.ngrams_dict:\n",
    "                    self.ngrams_dict[ngram_key] += 1\n",
    "                else:\n",
    "                    self.ngrams_dict[ngram_key] = 1\n",
    "\n",
    "    def get_frequency(self, key):\n",
    "        return self.ngrams_dict.get(\"###\".join(key), 0)\n",
    "\n",
    "    def get_high_freq_ngrams(self, min_freq):\n",
    "        return [k.split(\"###\") for (k,v) in self.ngrams_dict.items() if v >= min_freq]\n",
    "\n",
    "@memoized\n",
    "def synsets(token):\n",
    "    return wn.synsets(token)\n",
    "\n",
    "# information value based on corpus\n",
    "@memoized\n",
    "def info_value(word, corpus=word_stat):\n",
    "    n = corpus.get(word,1)\n",
    "    N = len(corpus)\n",
    "    return 1 - log(n + 1) / log(N + 1)\n",
    "\n",
    "# sentence similarity based\n",
    "def shallow_similarity(sentence_1, sentence_2, delta = 0.75):\n",
    "    tokens_1 = sentence_1.split()\n",
    "    tokens_2 = sentence_2.split()\n",
    "    tokens_all = list(set(tokens_1 + tokens_2))\n",
    "\n",
    "    sim = np.empty((len(tokens_1),len(tokens_all)))\n",
    "\n",
    "    for a in range(len(tokens_1)):\n",
    "        for b in range(len(tokens_all)):\n",
    "            s1 = synsets(tokens_1[a])\n",
    "            s2 = synsets(tokens_all[b])\n",
    "            if len(s1) > 0 and len(s2) > 0:\n",
    "                s1 = s1[0]\n",
    "                s2 = s2[0]\n",
    "                # TODO: memoize\n",
    "                simi = wup_similarity(s1, s2)\n",
    "                if simi is None or simi <= 0.25:\n",
    "                    simi = 0\n",
    "                #print tokens_1[a], tokens_all[b], simi, simi * info_value(s1) * info_value(s2)\n",
    "                sim[(a,b)] = simi * info_value(s1) * info_value(s2)\n",
    "            else:\n",
    "                sim[(a,b)] = 0\n",
    "                \n",
    "    max_sim = dict(zip(tokens_all,map(max, zip(*sim))))\n",
    "\n",
    "    s1 = [0 if k not in tokens_1 else v  for k,v in max_sim.items()]\n",
    "    s2 = [0 if k not in tokens_2 else v  for k,v in max_sim.items()]\n",
    "\n",
    "    Sd = cosine_distance(s1,s2)\n",
    "\n",
    "    r1 = np.array([0 if token not in tokens_1 else tokens_1.index(token) for i,token in enumerate(tokens_all)])\n",
    "    r2 = np.array([0 if token not in tokens_2 else tokens_2.index(token) for i,token in enumerate(tokens_all)])\n",
    "\n",
    "    Sr = 1 - np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2)\n",
    "\n",
    "    #print Sd, Sr\n",
    "    return delta * Sd + (1 - delta) * Sr\n",
    "\n",
    "# sentence similarity based (with synonyms)\n",
    "def deep_similarity(sentence_1, sentence_2, delta = 0.75):\n",
    "    tokens_1 = sentence_1.split()\n",
    "    tokens_2 = sentence_2.split()\n",
    "    tokens_all = list(set(tokens_1 + tokens_2))\n",
    "\n",
    "    sim = np.empty((len(tokens_1),len(tokens_all)))\n",
    "\n",
    "    for a in range(len(tokens_1)):\n",
    "        for b in range(len(tokens_all)):\n",
    "            w1 = tokens_1[a]\n",
    "            w2 = tokens_all[b]\n",
    "            \n",
    "            s1 = synonyms(w1)\n",
    "            s2 = synonyms(w2)\n",
    "\n",
    "            c1, c2 = w1, w2\n",
    "\n",
    "            if w1 == w2:\n",
    "                sim[(a,b)] = info_value(w1) * info_value(w2)\n",
    "    \n",
    "            elif len(s1) > 0 and len(s2) > 0:\n",
    "                max_simi = 0\n",
    "                for s1_synonym in s1:\n",
    "                    for s2_synonym in s2:\n",
    "                        # TODO: memoize\n",
    "                        simi = wn.wup_similarity(s1_synonym.synset, s2_synonym.synset)\n",
    "                        if simi > max_simi:\n",
    "                            max_simi = simi\n",
    "                        \n",
    "                if max_simi is None or max_simi <= 0.25:\n",
    "                    max_simi = 0\n",
    "\n",
    "                sim[(a,b)] = max_simi * info_value(w1) * info_value(w2)\n",
    "            else:\n",
    "                sim[(a,b)] = 0\n",
    "\n",
    "    max_sim = dict(zip(tokens_all,map(max, zip(*sim))))\n",
    "\n",
    "    s1 = [0 if k not in tokens_1 else v  for k,v in max_sim.items()]\n",
    "    s2 = [0 if k not in tokens_2 else v  for k,v in max_sim.items()]\n",
    "\n",
    "    Sd = cosine_distance(s1,s2)\n",
    "\n",
    "    r1 = np.array([0 if token not in tokens_1 else tokens_1.index(token) for i,token in enumerate(tokens_all)])\n",
    "    r2 = np.array([0 if token not in tokens_2 else tokens_2.index(token) for i,token in enumerate(tokens_all)])\n",
    "\n",
    "    Sr = 1 - np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2)\n",
    "\n",
    "    return delta * Sd + (1 - delta) * Sr\n",
    "\n",
    "\n",
    "def sentence_similarity(essays, output, essay_set = None, essay_type = None):\n",
    "    sentence_sim_output = open(output,\"w\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for index_essay, essay_1 in enumerate([essay for essay in essays.essays if \\\n",
    "                                           (essay_set is None or essay.essay_set == essay_set) and (essay_type is None or essay.essay_type == essay_type)]):\n",
    "        scores_mul = 0\n",
    "        scores_sum = 0\n",
    "\n",
    "        print index_essay, essay_1.id, time.time(), essay_1.score_1, essay_1.score_2\n",
    "        for sentence_1 in cut_text(essay_1.get_text(\"proc\"),10):\n",
    "            print sentence_1\n",
    "            max_similarity = [0,0,0,0]\n",
    "            max_similarity_sent = [\"\",\"\",\"\",\"\"]\n",
    "            essays_ids = [essay.id for essay in essays.essays if essay.id != essay_1.id and essay.essay_set == essay_1.essay_set and essay.score_1 is not None]\n",
    "            \n",
    "            for essay_num in random.sample(essays_ids, 300):\n",
    "                essay_2 = essays.get_essay_by_id(essay_num)\n",
    "                for sentence_2 in cut_text(essay_2.get_text(\"proc\"),10):\n",
    "                    same_words = set(sentence_1.split()).intersection(sentence_2.split()).difference(stopwords_set)\n",
    "                    if len(same_words) == 0:\n",
    "                        continue\n",
    "                    similarity = shallow_similarity(sentence_1, sentence_2)\n",
    "                    \n",
    "                    if similarity > max_similarity[essay_2.score_1]:\n",
    "                        max_similarity[essay_2.score_1] = similarity\n",
    "                        max_similarity_sent[essay_2.score_1] = sentence_2\n",
    "\n",
    "                # if very similar sentences found in 0-scored set and above\n",
    "                # it means that it is redundant\n",
    "                #if max_similarity[0] >= 0.75 and max_similarity[1] >= 0.75:\n",
    "                #    max_similarity = [0,0,0,0]\n",
    "                #    break\n",
    "                        \n",
    "            for sim_score, sent in zip(max_similarity,max_similarity_sent):\n",
    "                print sim_score, sent\n",
    "\n",
    "            sentence_sim_output.write(\"%s;%s\\n\" % (essay_1.id, \";\".join([str(k) for k in max_similarity])))\n",
    "                    \n",
    "        print time.time()\n",
    "\n",
    "    sentence_sim_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-seller",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
